{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6087a9-2873-4f39-ba30-66215aaf7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26039661-94d7-41fc-8ec2-d0c4bf7721ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "video_caps = open('/l/users/xinyue.li/caption/SwinBERT/output/results.json')\n",
    "video_caps = json.load(video_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481df4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_names_test = list(video_caps.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901cfa40-c86b-4f0d-99b1-395ad09b5245",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ca16a5-35a1-4e2e-a40f-5cf84aa02719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/lib/python3.11/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/fathinah.izzati/miniconda3/lib/python3.11/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda\n",
    "import llama\n",
    "from util.misc import *\n",
    "from data.utils import load_and_transform_audio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a870aaa-5b5d-494a-9177-18fcbeef9817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading llama\n",
      "Loading LLaMA-Adapter from ./ckpts/checkpoint.pth\n",
      "WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m-a-p/MERT-v1-330M were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v1-330M and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model args: ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=None, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-06, max_batch_size=1, max_seq_len=8192, w_bias=True, w_lora=True, lora_rank=16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/lib/python3.11/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403378171/work/torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "print('loading llama')\n",
    "model = './ckpts/checkpoint.pth'\n",
    "llama_dir = '/l/users/fathinah.izzati/ml711/llama'\n",
    "mert_path = 'm-a-p/MERT-v1-330M'\n",
    "knn_dir = './ckpts'\n",
    "llama_type = 'llama-2-7b-chat'\n",
    "model = llama.load(model_path=model, llama_dir=llama_dir, mert_path=mert_path, knn=True, knn_dir=knn_dir, llama_type=llama_type)\n",
    "print('Model LLAMA is loaded')\n",
    "model.eval()\n",
    "print('Model LLAMA is eval()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78335f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_generate(\n",
    "        audio_path,\n",
    "        audio_weight,\n",
    "        prompts,\n",
    "        cache_size,\n",
    "        cache_t,\n",
    "        cache_weight,\n",
    "        max_gen_len,\n",
    "        gen_t, top_p\n",
    "):\n",
    "    inputs = {}\n",
    "    audio = load_and_transform_audio_data([audio_path])\n",
    "    inputs['Audio'] = [audio, audio_weight]\n",
    "    text_output = None\n",
    "    with torch.cuda.amp.autocast():\n",
    "        results = model.generate(inputs, prompts, max_gen_len=max_gen_len, temperature=gen_t, top_p=top_p,\n",
    "                                     cache_size=cache_size, cache_t=cache_t, cache_weight=cache_weight)\n",
    "    text_output = results[0].strip()\n",
    "    return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(prompt):\n",
    "    prompts = [llama.format_prompt(prompt)]\n",
    "    prompts = [model.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "    return prompts\n",
    "# q = 'Describe the audio with at least 3 adjectives'\n",
    "q1 = 'Describe elements of the music, such as its mood, rhythm, and genre'\n",
    "prompts1 = encode_prompt(q1)\n",
    "q2 = 'Describe musical features from this audio in detail'\n",
    "prompts2 = encode_prompt(q2)\n",
    "q3 = 'Describe what you hear'\n",
    "prompts3 = encode_prompt(q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ad9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio in audio_names_test:\n",
    "    audio_path = f'/l/users/xinyue.li/caption/dataset/audios/{audio}.mp3'\n",
    "    print(audio_path)\n",
    "    print('Start generate')\n",
    "    output1 = multimodal_generate(audio_path, 1, prompts1, 100, 20.0, 0.0, 512, 0.6, 0.8)\n",
    "    output2 = multimodal_generate(audio_path, 1, prompts2, 100, 20.0, 0.0, 512, 0.6, 0.8)\n",
    "    output3 = multimodal_generate(audio_path, 1, prompts3, 100, 20.0, 0.0, 512, 0.6, 0.8)\n",
    "    print('Success')\n",
    "    print(f\"Audio File: {audio_path}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {output}\")\n",
    "    outputs1 = {}\n",
    "    outputs2 = {}\n",
    "    outputs3 = {}\n",
    "    outputs1[audio]={'pred':output1, 'q':q}\n",
    "    outputs2[audio]={'pred':output2, 'q':q}\n",
    "    outputs3[audio]={'pred':output3, 'q':q}\n",
    "    a =  open('mullama_output1.json')\n",
    "    b =  open('mullama_output2.json')\n",
    "    c =  open('mullama_output3.json')\n",
    "    z1 = json.load(a)\n",
    "    z2 = json.load(b)\n",
    "    z3 = json.load(c)\n",
    "    z1.update(outputs1)\n",
    "    z2.update(outputs2)\n",
    "    z3.update(outputs3)\n",
    "    with open('mullama_output1.json', 'w', encoding ='utf8') as x: \n",
    "        json.dump(z1, x)\n",
    "    with open('mullama_output2.json', 'w', encoding ='utf8') as x: \n",
    "        json.dump(z2, x)\n",
    "    with open('mullama_output3.json', 'w', encoding ='utf8') as x: \n",
    "        json.dump(z3, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32bda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_caps = open('mullama_output.json')\n",
    "# audio_caps = json.load(audio_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1a93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
